{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68c90bb",
   "metadata": {},
   "source": [
    "# A Tour of Transformer Applications via Hugging Face\n",
    "- All of the models that we have used are already fine-tuned for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45c637",
   "metadata": {},
   "source": [
    "### The Hugging Face Ecosystem\n",
    "- The Hugging Face Ecosystem consists of mainly two parts:\n",
    "    - A family of libraries: provide the code\n",
    "    - The Hub: provides the pretrained model weights, datasets, evaluation metrics, and more.\n",
    "- The Hugging Face Hub\n",
    "    - The Hugging Face Hub hosts over 20,000 freely available models.\n",
    "    - You can search for those using filtering for Tasks, Frameworks, and Datasets, and load them with one line of code!\n",
    "- The Hugging Face Datasets\n",
    "    - The Hugging Face Datasets simplifies loading, processing, and storing datasets, by standard interface and smart caching. (You don't have to redo your preprocessing each time you run the code.)\n",
    "    - Avoids RAM limitations by memory mapping, which stores the contents of a file in virtual memory and enables multiple processes to modify a file more efficeintly.\n",
    "- The Hugging Face Accelerate\n",
    "    - You can run your raw PyTorch training scripts on any kind of device.\n",
    "    - Accelerate adds a layer of abstraction to your normal training loops, which takes care of all the custom logic necessary for the training intrastructure.\n",
    "    - Accelerator.device automatically detects and selects the best available hardware(CPU, GPU, TPU) on your system.\n",
    "    - Parallel learning can be easily performed on multiple GPUs or TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987ecf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5440fe1",
   "metadata": {},
   "source": [
    "### Required Packages:\n",
    "- datasets, pandas, transformers, torch, accelerate, numpy, scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40fbd28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification') # pipeline은 굉장히 abstracting level이 높은 API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f94ce",
   "metadata": {},
   "source": [
    "### A simple classifier\n",
    "- Let's generate some prediction.\n",
    "- Each pipeline takes a string of text as input, and returns a list of predictions (list of dictionary))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0964991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label     score\n",
      "0  NEGATIVE  0.901546\n"
     ]
    }
   ],
   "source": [
    "# Sentimental Analysis\n",
    "outputs = classifier(text)\n",
    "print(pd.DataFrame(outputs)) # The model is very confident that the text has a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6292a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score           word  start  end\n",
      "0          ORG  0.879010         Amazon      5   11\n",
      "1         MISC  0.990859  Optimus Prime     36   49\n",
      "2          LOC  0.999755        Germany     90   97\n",
      "3         MISC  0.556569           Mega    208  212\n",
      "4          PER  0.590257         ##tron    212  216\n",
      "5          ORG  0.669692         Decept    253  259\n",
      "6         MISC  0.498349        ##icons    259  264\n",
      "7         MISC  0.775361       Megatron    350  358\n",
      "8         MISC  0.987854  Optimus Prime    367  380\n",
      "9          PER  0.812096      Bumblebee    502  511\n"
     ]
    }
   ],
   "source": [
    "# Named-Entity Recognition (NER): Named Entity(이름을 가진 개체)를 Recognition(인식)하는 것\n",
    "ner_tagger = pipeline('ner', aggregation_strategy='simple')\n",
    "# \"Simple\" aggregation strategy merges tokens that have the same entity tag (i.e., Optimus Prime)\n",
    "# When they have different predictions, they will be treated separately. (i.e., Mega / ##tron)\n",
    "outputs = ner_tagger(text)\n",
    "print(pd.DataFrame(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dda7a6",
   "metadata": {},
   "source": [
    "- Output\n",
    "    - ORG: Organization\n",
    "    - LOC: Location\n",
    "    - PER: Person\n",
    "    - MISC: Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91981e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  start  end                   answer\n",
      "0  0.631292    335  358  an exchange of Megatron\n"
     ]
    }
   ],
   "source": [
    "# (Extractive) Question Answering\n",
    "reader = pipeline('question-answering')\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "print(pd.DataFrame([outputs])) # Start, end: The character indices where the answer span was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a605a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729141809.072661 4887093 service.cc:146] XLA service 0x6000017ce400 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1729141809.074595 4887093 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
      "2024-10-17 14:10:09.164324: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1729141809.518361 4887093 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last week, I ordered an Optimus Prime action figure from your online store in germany. when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "summarizer = pipeline('summarization')\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a06f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5313b59a5ccd4092a48d2b7c5204d2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0fdd32f27d42fca06d0541309cf53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df3e872399a4683be54c7da440bb9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/min_leon/anaconda3/envs/mp_vision/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur bestellte ich. Anbei sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, von Ihnen bald zu hören. Aufrichtig, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "# Translation\n",
    "translator = pipeline('translation_en_to_de', model='Helsinki-NLP/opus-mt-en-de') # able to specify model \n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cbc9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bublebee, I am sorry to hear that your order was mixed up. My order has been processed and I am confident that it will be delivered tomorrow. If you would just like to express your concerns about your own package or wish to contact me for further information, visit Amazon.\n",
      "\n",
      "\n",
      "For more information, please visit the Amazon.com website.\n"
     ]
    }
   ],
   "source": [
    "# Text generation (autocomplete, fast reply to customer feedback)\n",
    "# RAG\n",
    "generator = pipeline('text-generation')\n",
    "response = \"Dear Bublebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MP_vision",
   "language": "python",
   "name": "mp_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
